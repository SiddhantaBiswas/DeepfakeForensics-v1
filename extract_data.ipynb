{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from src.preprocessing_util import *\n",
    "from src.util import create_train_test_sets\n",
    "from facenet_pytorch import MTCNN\n",
    "import torch\n",
    "import pandas as pd\n",
    "from shutil import copyfile\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First we need to setup data folders\n",
    "\n",
    "Required:\n",
    "\n",
    "1. Path to raw data\n",
    "2. Path to store non-augmented data\n",
    "3. Path to store augmented data\n",
    "\n",
    "The raw data directory should contain two subfolders, called \"real\" and \"fake\", and should hold the videos belonging to that category. So in the special case of the Celeb-DF:\n",
    "\n",
    "Place the videos from \"Celeb-real\" and \"YouTube-real\" into the \"real\" folder. \\\n",
    "Place the videos from \"Celeb-synthesis\" into the \"fake\" folder.\n",
    "\n",
    "We specify whether the derived dataset should be aimed at training temporal, or non-temporal models. Currently, both model types can only be trained using their respective dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_raw_data = 'raw_data/' # path to raw data\n",
    "\n",
    "temporal = False\n",
    "\n",
    "path_to_store_faces = f'data/{\"temporal\" if temporal else \"nontemp\"}/faces/'           # path to store non-augmented data\n",
    "path_to_store_faces_aug = f'data/{\"temporal\" if temporal else \"nontemp\"}/faces_aug/'   # path to store augmented data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First load the face detection module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load face detector\n",
    "face_detector = MTCNN(image_size=224, margin=10, keep_all=False, device=device, post_process=False).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config\n",
    "Specify the amount of frames extracted per real file. If this is adapted, this needs to be changed as well separately for fake files.\n",
    "\n",
    "E.g. for the Celeb-DF dataset:\n",
    "\n",
    "There are 890 real files: 890 * 65 = 57.850\n",
    "There are 5.639 fake files: 5.639 * 10 = 56.390\n",
    "\n",
    "So the current configuration results in a balanced dataset. Note that only multiples of 5 can be selected when extracting temporal data, because the sequence length for the face sequences is set to 5 for the LSTM.\n",
    "\n",
    "Also, a minimum face detection threshold can be set to disregard files which result in a large proportion of frames not detecting any faces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_frames = 5\n",
    "min_face_cutoff = 0\n",
    "\n",
    "# Load facial detection pipeline\n",
    "face_detection = FaceDetection(face_detector, device, n_frames=n_frames)\n",
    "\n",
    "# enable logging plots, if this is true, no face images will be saved, just the plots\n",
    "log_plots = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the dataset\n",
    "Extract one subfolder after another. We keep track of the labels for each datapoint via stored csv files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting  faces from 100 Real files\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aea40cc1fb74521bd07f79bfeff72e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# real folder\n",
    "path_to_folder =  path_to_raw_data + 'real/'\n",
    "name_csv = 'real'\n",
    "label = 'Real'\n",
    "\n",
    "labels = get_CDF_per_folder(path_to_data=path_to_folder,\n",
    "                            path_to_store_faces=path_to_store_faces,\n",
    "                            path_to_store_faces_aug=path_to_store_faces_aug,\n",
    "                            face_detection=face_detection,\n",
    "                            label=label,\n",
    "                            csv_file_name=name_csv,\n",
    "                            min_face_cutoff=min_face_cutoff,\n",
    "                            temporal = temporal,\n",
    "                            log_plots=log_plots,\n",
    "                            verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now derived data from all real files. Next we need to derive data from the fake files.\n",
    "As mentioned, this needs to be done with different number of frames per file, to ensure a balanced dataset.\n",
    "Make sure both n_frame instances produce an even amount of datapoints (see above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to initialize a new face detection model for smaller amount of frames\n",
    "n_frames = 5\n",
    "min_face_cutoff = 0\n",
    "face_detection = FaceDetection(face_detector, device, n_frames=n_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting  faces from 116 Fake files\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "234f1a0063b944839f42475a0a6001ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=116.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# synthesis folder\n",
    "path_to_folder =  path_to_raw_data + 'fake/'\n",
    "name_csv = 'fake'\n",
    "label = 'Fake'\n",
    "\n",
    "labels = get_CDF_per_folder(path_to_data=path_to_folder,\n",
    "                            path_to_store_faces=path_to_store_faces,\n",
    "                            path_to_store_faces_aug=path_to_store_faces_aug,\n",
    "                            face_detection=face_detection,\n",
    "                            label=label,\n",
    "                            csv_file_name=name_csv,\n",
    "                            min_face_cutoff=min_face_cutoff,\n",
    "                            temporal = temporal,\n",
    "                            log_plots=log_plots,\n",
    "                            verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label merging\n",
    "For each folder, we have a respective label file. Those need to be merged.\n",
    "For each dataset, we handle two different types of labels.\n",
    "\n",
    "1. Per file labels\n",
    "2. Per datapoint labels (per face image for non-temporal models, per face-window for temporal models)\n",
    "\n",
    "\n",
    "First we merge the labels on file-level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>003.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>012.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>020.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>025.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>df_95.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>df_96.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>df_97.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>df_98.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>df_99.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>216 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          file  label\n",
       "0      000.mp4      0\n",
       "1      003.mp4      0\n",
       "2      012.mp4      0\n",
       "3      020.mp4      0\n",
       "4      025.mp4      0\n",
       "..         ...    ...\n",
       "211  df_95.mp4      1\n",
       "212  df_96.mp4      1\n",
       "213  df_97.mp4      1\n",
       "214  df_98.mp4      1\n",
       "215  df_99.mp4      1\n",
       "\n",
       "[216 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"Labels/\"\n",
    "file1 = \"real_labels_per_file.csv\"\n",
    "file2 = \"fake_labels_per_file.csv\"\n",
    "\n",
    "\n",
    "\n",
    "labels_per_file = combine_labels(path, file1, file2)\n",
    "labels_per_file.to_csv(\"Labels/labels_per_file.csv\")\n",
    "labels_per_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we merge the labels on datapoint-level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000_000.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001_000.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002_000.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>003_000.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>004_000.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1075</th>\n",
       "      <td>000_df_99.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1076</th>\n",
       "      <td>001_df_99.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1077</th>\n",
       "      <td>002_df_99.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1078</th>\n",
       "      <td>003_df_99.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1079</th>\n",
       "      <td>004_df_99.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1080 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               file  label\n",
       "0       000_000.mp4      0\n",
       "1       001_000.mp4      0\n",
       "2       002_000.mp4      0\n",
       "3       003_000.mp4      0\n",
       "4       004_000.mp4      0\n",
       "...             ...    ...\n",
       "1075  000_df_99.mp4      1\n",
       "1076  001_df_99.mp4      1\n",
       "1077  002_df_99.mp4      1\n",
       "1078  003_df_99.mp4      1\n",
       "1079  004_df_99.mp4      1\n",
       "\n",
       "[1080 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"Labels/\"\n",
    "file1 = f\"real_labels_per_{'face' if not temporal else 'face_window'}.csv\"\n",
    "file2 = f\"fake_labels_per_{'face' if not temporal else 'face_window'}.csv\"\n",
    "\n",
    "labels_per_face = combine_labels(path, file1, file2)\n",
    "labels_per_face.to_csv(f\"Labels/labels_per_{'face' if not temporal else 'face_window'}.csv\")\n",
    "labels_per_face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create splits\n",
    "\n",
    "Now that we have the datasets and labels, we can perform the train/val/test split. We do this on file level.\n",
    "For this, we can adapt the size of the training set. The resulting proportion of the dataset will be evenly split into validation/testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the split. The resulting label files per split will be stored in the root folders where the data is located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deriving the correct face labels for the split...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a87e557d1ff46a6b6507ecf41b12f49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=172.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Per face labelling derived for split:\n",
      "               file  label\n",
      "0     003_df_11.mp4      1\n",
      "1     002_df_11.mp4      1\n",
      "2     000_df_11.mp4      1\n",
      "3     001_df_11.mp4      1\n",
      "4     004_df_11.mp4      1\n",
      "..              ...    ...\n",
      "855  002_df_101.mp4      1\n",
      "856  001_df_101.mp4      1\n",
      "857  004_df_101.mp4      1\n",
      "858  003_df_101.mp4      1\n",
      "859  000_df_101.mp4      1\n",
      "\n",
      "[860 rows x 2 columns]\n",
      "\n",
      "Per window labelling for split:\n",
      "           file  label\n",
      "152   df_11.mp4      1\n",
      "107   df_27.mp4      1\n",
      "119   df_41.mp4      1\n",
      "47      042.mp4      0\n",
      "135   df_88.mp4      1\n",
      "..          ...    ...\n",
      "35      774.mp4      0\n",
      "164   df_76.mp4      1\n",
      "172     187.mp4      0\n",
      "73    df_21.mp4      1\n",
      "199  df_101.mp4      1\n",
      "\n",
      "[172 rows x 2 columns]\n",
      "Finished split train/!\n",
      "Deriving the correct face labels for the split...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55183368c7e945b6aab042bb9b1ea367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=22.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Per face labelling derived for split:\n",
      "            file  label\n",
      "0    004_053.mp4      0\n",
      "1    001_053.mp4      0\n",
      "2    000_053.mp4      0\n",
      "3    002_053.mp4      0\n",
      "4    003_053.mp4      0\n",
      "..           ...    ...\n",
      "105  000_857.mp4      0\n",
      "106  001_857.mp4      0\n",
      "107  004_857.mp4      0\n",
      "108  002_857.mp4      0\n",
      "109  003_857.mp4      0\n",
      "\n",
      "[110 rows x 2 columns]\n",
      "\n",
      "Per window labelling for split:\n",
      "          file  label\n",
      "136    053.mp4      0\n",
      "116    922.mp4      0\n",
      "124    003.mp4      0\n",
      "94   df_87.mp4      1\n",
      "86   df_75.mp4      1\n",
      "162  df_51.mp4      1\n",
      "55     846.mp4      0\n",
      "151  df_63.mp4      1\n",
      "182    808.mp4      0\n",
      "108  df_15.mp4      1\n",
      "130    940.mp4      0\n",
      "169  df_22.mp4      1\n",
      "74     845.mp4      0\n",
      "146  df_99.mp4      1\n",
      "3    df_24.mp4      1\n",
      "178    492.mp4      0\n",
      "186  df_79.mp4      1\n",
      "38     025.mp4      0\n",
      "32   df_83.mp4      1\n",
      "134  df_98.mp4      1\n",
      "57   df_69.mp4      1\n",
      "208    857.mp4      0\n",
      "Finished split val/!\n",
      "Deriving the correct face labels for the split...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ed98f8aac314da2ae3f22ce4a565b5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=22.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Per face labelling derived for split:\n",
      "              file  label\n",
      "0    003_df_72.mp4      1\n",
      "1    001_df_72.mp4      1\n",
      "2    004_df_72.mp4      1\n",
      "3    000_df_72.mp4      1\n",
      "4    002_df_72.mp4      1\n",
      "..             ...    ...\n",
      "105  004_df_62.mp4      1\n",
      "106  002_df_62.mp4      1\n",
      "107  003_df_62.mp4      1\n",
      "108  001_df_62.mp4      1\n",
      "109  000_df_62.mp4      1\n",
      "\n",
      "[110 rows x 2 columns]\n",
      "\n",
      "Per window labelling for split:\n",
      "           file  label\n",
      "95    df_72.mp4      1\n",
      "179   df_47.mp4      1\n",
      "40      745.mp4      0\n",
      "5     df_45.mp4      1\n",
      "45    df_56.mp4      1\n",
      "69    df_71.mp4      1\n",
      "166     413.mp4      0\n",
      "19    df_57.mp4      1\n",
      "201   df_10.mp4      1\n",
      "157     418.mp4      0\n",
      "170     273.mp4      0\n",
      "188   df_19.mp4      1\n",
      "138     735.mp4      0\n",
      "91      391.mp4      0\n",
      "39      546.mp4      0\n",
      "175   df_53.mp4      1\n",
      "137    df_4.mp4      1\n",
      "8    df_111.mp4      1\n",
      "6       807.mp4      0\n",
      "90   df_110.mp4      1\n",
      "56   df_106.mp4      1\n",
      "189   df_62.mp4      1\n",
      "Finished split test/!\n"
     ]
    }
   ],
   "source": [
    "labels_per_file = 'Labels/labels_per_file.csv'\n",
    "labels_per_face = 'Labels/labels_per_face.csv'\n",
    "\n",
    "\n",
    "create_train_test_sets(labels_per_file=labels_per_file,\n",
    "                       labels_per_face=labels_per_face,\n",
    "                       root_dir=path_to_store_faces,\n",
    "                       root_dir_aug=path_to_store_faces_aug,\n",
    "                       train_size=train_size,\n",
    "                       temporal=temporal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, because we in the end, we want to predict on video-level, we create a subfolder in our datafolder holding the video sequences associated with the testsplit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Frede\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:11: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53f8451a2afe4f7faa51fcf9e43da956",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "testlabels = pd.read_csv(f\"data/{'temporal' if temporal else 'nontemp'}/faces/testlabels_per_file.csv\" , index_col=0) \n",
    "\n",
    "target_dir_real = f\"data/{'temporal' if temporal else 'nontemp'}/testfiles/real/\"\n",
    "target_dir_fake = f\"data/{'temporal' if temporal else 'nontemp'}/testfiles/fake/\"\n",
    "\n",
    "for subpath in [target_dir_real, target_dir_fake]:                \n",
    "    if not os.path.exists(subpath):\n",
    "        os.makedirs(subpath)\n",
    "\n",
    "\n",
    "for row in tqdm(testlabels.iterrows()): \n",
    "    target_dir = target_dir_fake if row[1][1] == 1 else target_dir_real\n",
    "    try:\n",
    "        copyfile(path_to_raw_data + 'real/' + row[1][0], target_dir + row[1][0])\n",
    "    except FileNotFoundError:\n",
    "        try:\n",
    "            copyfile(path_to_raw_data + 'fake/' + row[1][0], target_dir + row[1][0])\n",
    "        except FileNotFoundError:\n",
    "            print(f'whoops, did not find file {row[1][0]} at all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
